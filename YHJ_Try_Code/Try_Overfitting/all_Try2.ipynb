{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u__e9EBbVaol"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq2DancQVJC6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# check the current path\n",
        "print(os.getcwd()) # /content\n",
        "\n",
        "# You should copy the path : 왼쪽 폴더에서 오른쪽 버튼 후 경로복사\n",
        "os.chdir('/content/gdrive/MyDrive/Colab Notebooks/2024_OSP_SW')\n",
        "\n",
        "print(os.getcwd()) # path has been changed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YO_KRI0XVcWS"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 설치\n",
        "!pip install efficientnet-pytorch\n",
        "!pip install albumentations\n",
        "!pip install timm\n",
        "!pip install scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7NH1aJcsVd8d"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Pa7qBErBVf7Q"
      },
      "outputs": [],
      "source": [
        "!pip install facenet-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeWEbHWPVgWV"
      },
      "source": [
        "# 2. 데이터 로드 및 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VItPDWv5VoZX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n",
        "input_dirs = [\n",
        "    \"/content/gdrive/MyDrive/Colab Notebooks/2024_OSP_SW/dfdc_train_part_0\",\n",
        "    \"/content/gdrive/MyDrive/Colab Notebooks/2024_OSP_SW/dfdc_train_part_1\"\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for input_dir in input_dirs:\n",
        "    json_file = os.path.join(input_dir, \"metadata.json\")\n",
        "    with open(json_file, \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    df = pd.DataFrame.from_dict(metadata, orient=\"index\").reset_index()\n",
        "    df.columns = [\"filename\", \"label\", \"split\", \"original\"]\n",
        "    df[\"label\"] = df[\"label\"].map({\"FAKE\": 0, \"REAL\": 1})\n",
        "    dfs.append(df)\n",
        "\n",
        "# 데이터셋 합치기\n",
        "df_combined = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# 라벨 통계 출력\n",
        "real_count = (df_combined[\"label\"] == 1).sum()\n",
        "fake_count = (df_combined[\"label\"] == 0).sum()\n",
        "\n",
        "print(f\"REAL videos: {real_count}\")\n",
        "print(f\"FAKE videos: {fake_count}\")\n",
        "\n",
        "# 데이터 균형 유지 및 Train/Test Split\n",
        "df_label_1 = df_combined[df_combined[\"label\"] == 1]\n",
        "df_label_0 = df_combined[df_combined[\"label\"] == 0]\n",
        "\n",
        "n_samples = len(df_label_1)\n",
        "df_label_0_sampled = resample(df_label_0, replace=False, n_samples=n_samples, random_state=42)\n",
        "\n",
        "df_balanced = pd.concat([df_label_1, df_label_0_sampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "train_df, test_df = train_test_split(df_balanced, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noj5cT7RVscI"
      },
      "source": [
        "# 3. 데이터셋 클래스 및 DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cosnwjiyVtJ8"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from facenet_pytorch import MTCNN\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# MTCNN 모델 초기화\n",
        "mtcnn = MTCNN(keep_all=False)\n",
        "\n",
        "# 처리된 비디오 출력 경로\n",
        "output_dir = \"/content/gdrive/MyDrive/Colab Notebooks/2024_OSP_SW/all2_2_processed_videos\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "skipped_videos = []\n",
        "\n",
        "# 비디오 처리 함수\n",
        "def process_video(video_info):\n",
        "    video_path, output_path = video_info\n",
        "    if not os.path.exists(video_path):\n",
        "        return output_path\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Skipping invalid video: {video_path}\")\n",
        "        skipped_videos.append(video_path)\n",
        "        return None\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        box, _ = mtcnn.detect(Image.fromarray(rgb_frame))\n",
        "        if box is not None:\n",
        "            x1, y1, x2, y2 = map(int, box[0])\n",
        "            if x1 < 0 or y1 < 0 or x2 < 0 or y2 < 0:\n",
        "                continue\n",
        "            cropped_face = frame[y1:y2, x1:x2]\n",
        "            cropped_face_resized = cv2.resize(cropped_face, (224, 224))\n",
        "            cv2.imwrite(output_path, cropped_face_resized)\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    return output_path\n",
        "\n",
        "# 모든 데이터셋 처리\n",
        "for input_dir in input_dirs:\n",
        "    video_info_list = [\n",
        "        (os.path.join(input_dir, filename), os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}.jpg\"))\n",
        "        for filename in os.listdir(input_dir) if filename.endswith(\".mp4\")\n",
        "    ]\n",
        "    print(f\"Processing videos in {input_dir}...\")\n",
        "    for video_info in tqdm(video_info_list):\n",
        "        process_video(video_info)\n",
        "print(\"Processing complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORYfALhzVwtL"
      },
      "source": [
        "# 4. 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fNfdm7eVxTf"
      },
      "outputs": [],
      "source": [
        "from timm import create_model\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomEfficientNetB0(nn.Module):\n",
        "    def __init__(self, pretrained=True, num_classes=1):\n",
        "        super(CustomEfficientNetB0, self).__init__()\n",
        "        self.backbone = create_model(\"efficientnet_b0\", pretrained=pretrained)\n",
        "        self.backbone.reset_classifier(0)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.fc = nn.Linear(1280, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone.forward_features(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.mean(dim=[2, 3])\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class CustomConvNextTiny(nn.Module):\n",
        "    def __init__(self, pretrained=True, num_classes=1):\n",
        "        super(CustomConvNextTiny, self).__init__()\n",
        "        self.backbone = create_model(\"convnext_tiny\", pretrained=pretrained)\n",
        "        self.backbone.reset_classifier(0)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.fc = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone.forward_features(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.mean(dim=[2, 3])\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj2qEz9cV4UU"
      },
      "source": [
        "# 5. 학습 및 테스트 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJlPTAzsV6R8"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from timm import create_model\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, optimizer, scheduler, train_loader, num_epochs):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device).unsqueeze(1).float()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "def test_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    test_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device).unsqueeze(1).float()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            predictions = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
        "            y_pred.extend(predictions.flatten())\n",
        "            y_true.extend(labels.cpu().numpy().flatten())\n",
        "\n",
        "    accuracy = sum(1 for a, b in zip(y_true, y_pred) if a == b) / len(y_true) * 100\n",
        "    return {\"Test Loss\": test_loss / len(test_loader), \"Accuracy\": accuracy}, y_true, y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPR0X4WLV7oM"
      },
      "source": [
        "# 6. 모델 학습 및 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztr_LFCNj_2D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class DeepFakeDataset(Dataset):\n",
        "    def __init__(self, df, image_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, f\"{os.path.splitext(self.df.iloc[idx]['filename'])[0]}.jpg\")\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        label = self.df.iloc[idx]['label']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 처리된 비디오 출력 경로\n",
        "output_dir = \"/content/gdrive/MyDrive/Colab Notebooks/2024_OSP_SW/all2_processed_videos\"\n",
        "\n",
        "# 'cell 3'에서 처리된 이미지가 저장된 디렉토리로 설정\n",
        "image_dir = output_dir\n",
        "\n",
        "# 이미지 변환을 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 데이터셋을 생성\n",
        "train_dataset = DeepFakeDataset(train_df, image_dir, transform)\n",
        "test_dataset = DeepFakeDataset(test_df, image_dir, transform)\n",
        "\n",
        "# 데이터 로더를 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # 배치 크기 조정하며 변경\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) # 배치 크기 조정하며 변경\n",
        "\n",
        "# 사용할 장치를 정의\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPPw6kyYV9GW"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "efficientnet_b0 = CustomEfficientNetB0()\n",
        "optimizer_eff = optim.Adam(efficientnet_b0.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler_eff = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_eff, mode=\"min\", factor=0.5, patience=2)\n",
        "\n",
        "print(\"Training EfficientNet-B0...\")\n",
        "train(efficientnet_b0, optimizer_eff, scheduler_eff, train_loader, num_epochs=10)\n",
        "\n",
        "print(\"\\nTesting EfficientNet-B0...\")\n",
        "metrics_eff, y_true_eff, y_pred_eff = test_model(efficientnet_b0, test_loader, device)\n",
        "print(metrics_eff)\n",
        "\n",
        "convnext_tiny = CustomConvNextTiny()\n",
        "optimizer_conv = optim.Adam(convnext_tiny.parameters(), lr=5e-6, weight_decay=1e-4)\n",
        "scheduler_conv = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_conv, mode=\"min\", factor=0.5, patience=2)\n",
        "\n",
        "print(\"Training ConvNextTiny...\")\n",
        "train(convnext_tiny, optimizer_conv, scheduler_conv, train_loader, num_epochs=10)\n",
        "\n",
        "print(\"\\nTesting ConvNextTiny...\")\n",
        "metrics_conv, y_true_conv, y_pred_conv = test_model(convnext_tiny, test_loader, device)\n",
        "print(metrics_conv)\n",
        "\n",
        "# 7. 혼동 행렬 시각화\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names=[\"FAKE\", \"REAL\"]):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(y_true_eff, y_pred_eff)\n",
        "plot_confusion_matrix(y_true_conv, y_pred_conv)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
