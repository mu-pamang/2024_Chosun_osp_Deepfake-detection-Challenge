{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch and Albumentations for image classification\n",
        "\n",
        "Modified for Colab based on https://albumentations.ai/docs/examples/pytorch_classification/"
      ],
      "metadata": {
        "id": "jQHHbyt7kavf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###필요한 library를 임포팅하기"
      ],
      "metadata": {
        "id": "p0-XVnb9kr9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MZWhYevoFAy",
        "outputId": "b7145973-6ce6-4327-c3bd-0ff26d3ab5d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.4.21-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Collecting albucore==0.0.20 (from albumentations)\n",
            "  Downloading albucore-0.0.20-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.20->albumentations) (3.10.6)\n",
            "Collecting simsimd>=5.9.2 (from albucore==0.0.20->albumentations)\n",
            "  Downloading simsimd-5.9.10-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Downloading albumentations-1.4.21-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.9/227.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.20-py3-none-any.whl (12 kB)\n",
            "Downloading simsimd-5.9.10-cp310-cp310-manylinux_2_28_x86_64.whl (680 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.7/680.7 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\n",
            "  Attempting uninstall: albucore\n",
            "    Found existing installation: albucore 0.0.19\n",
            "    Uninstalling albucore-0.0.19:\n",
            "      Successfully uninstalled albucore-0.0.19\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.4.20\n",
            "    Uninstalling albumentations-1.4.20:\n",
            "      Successfully uninstalled albumentations-1.4.20\n",
            "Successfully installed albucore-0.0.20 albumentations-1.4.21 simsimd-5.9.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X3bNwoZrkNvh"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import copy\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "\n",
        "cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###데이터세트를 다운로드하고 언팩하는 함수 정의"
      ],
      "metadata": {
        "id": "O0kEtdRwkuqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TqdmUpTo(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "\n",
        "def download_url(url, filepath):\n",
        "    directory = os.path.dirname(os.path.abspath(filepath))\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    if os.path.exists(filepath):\n",
        "        print(\"Filepath already exists. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    with TqdmUpTo(unit=\"B\", unit_scale=True, unit_divisor=1024, miniters=1, desc=os.path.basename(filepath)) as t:\n",
        "        urlretrieve(url, filename=filepath, reporthook=t.update_to, data=None)\n",
        "        t.total = t.n\n",
        "\n",
        "\n",
        "def extract_archive(filepath):\n",
        "    extract_dir = os.path.dirname(os.path.abspath(filepath))\n",
        "    shutil.unpack_archive(filepath, extract_dir)"
      ],
      "metadata": {
        "id": "GM63w9jykich"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cats vs Dogs 데이터세트 다운로드 후 추출: 고양이와 강아지를 분류하는 이진 분류(Binary Classification) 문제\n",
        "\n",
        "2024_OSP_SW 폴더 아래에 datasets 폴더를 만들어 놓고 실행할 것..."
      ],
      "metadata": {
        "id": "8hA0cURak842"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/2024_OSP_SW/datasets\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gqqiInvo8PY",
        "outputId": "22fd68e8-89f6-4ced-e568-fb04c64bc66c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/2024_OSP_SW/datasets\n",
            " cats-and-dogs.zip   CDLA-Permissive-2.0.pdf   PetImages  'readme[1].txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###처음 한번만 아래 코드 실행해서 다운로드 및 압축해제할 것\n",
        "\n",
        "is_downloaded = False (처음 한번)\n",
        "\n",
        "일단, 다운로드 후 압축해제해서 파일이 생성되면\n",
        "\n",
        "is_downloaded = True\n",
        "\n",
        "로 해서 다음 번에 재 실행할 경우 다시 다운로드 및 압축해제하지 않도록 함"
      ],
      "metadata": {
        "id": "c_8zUGBMz9H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "is_downloaded = True\n",
        "\n",
        "if is_downloaded == False:\n",
        "    !wget --no-check-certificate \\\n",
        "        \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\" \\\n",
        "        -O \"/content/drive/MyDrive/Colab Notebooks/2024_OSP_SW/datasets/cats-and-dogs.zip\"\n",
        "\n",
        "    extract_archive(\"/content/drive/MyDrive/Colab Notebooks/2024_OSP_SW/datasets/cats-and-dogs.zip\")"
      ],
      "metadata": {
        "id": "-fEtmavUsmoG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###데이터세트의 파일을 학습용과 검증용으로 나누기\n",
        "\n",
        "폴더 구조가 Cat과 Dog로 나뉘어 있으므로 이걸로 자연스럽게 0과 1의 라벨을 만들 수 있음.\n",
        "DeepFake 동영상의 경우 metadata.json 파일에서 label을 구별해야 하므로\n",
        "\n",
        "1. 라벨을 읽어서 0(real)이나 1(Fake)로 지정하거나\n",
        "2. 폴더를 REAL과 FAKE로 만들어서 해당 동영상을 옮기고 아래와 같은 방식으로 라벨 등을 처리하는\n",
        "\n",
        "두가지 방식 중 하나를 구현해야 함."
      ],
      "metadata": {
        "id": "UAcdHdoclmuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = \"/content/drive/MyDrive/Colab Notebooks/2024_OSP_SW/datasets/\"\n",
        "root_directory = os.path.join(dataset_directory, \"PetImages\")\n",
        "\n",
        "cat_directory = os.path.join(root_directory, \"Cat\")\n",
        "dog_directory = os.path.join(root_directory, \"Dog\")\n",
        "\n",
        "cat_images_filepaths = sorted([os.path.join(cat_directory, f) for f in os.listdir(cat_directory)])\n",
        "dog_images_filepaths = sorted([os.path.join(dog_directory, f) for f in os.listdir(dog_directory)])\n",
        "images_filepaths = [*cat_images_filepaths, *dog_images_filepaths]\n",
        "correct_images_filepaths = [i for i in images_filepaths if cv2.imread(i) is not None]\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(correct_images_filepaths)\n",
        "train_images_filepaths = correct_images_filepaths[:20000]\n",
        "val_images_filepaths = correct_images_filepaths[20000:-10]\n",
        "test_images_filepaths = correct_images_filepaths[-10:]\n",
        "print(len(train_images_filepaths), len(val_images_filepaths), len(test_images_filepaths))"
      ],
      "metadata": {
        "id": "HgDMhkmVlm6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###이미지와 라벨을 표시해주는 함수"
      ],
      "metadata": {
        "id": "UCgK2VMxlxV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image_grid(images_filepaths, predicted_labels=(), cols=5):\n",
        "    rows = len(images_filepaths) // cols\n",
        "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
        "    for i, image_filepath in enumerate(images_filepaths):\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        true_label = os.path.normpath(image_filepath).split(os.sep)[-2]\n",
        "        predicted_label = predicted_labels[i] if predicted_labels else true_label\n",
        "        color = \"green\" if true_label == predicted_label else \"red\"\n",
        "        ax.ravel()[i].imshow(image)\n",
        "        ax.ravel()[i].set_title(predicted_label, color=color)\n",
        "        ax.ravel()[i].set_axis_off()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cOlLCFq9lxM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image_grid(test_images_filepaths)"
      ],
      "metadata": {
        "id": "kwlsxpnhl87P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pytorch 데이터세트 클래스 정의\n",
        "\n",
        "실제로 라벨은 이 함수가 생성하며 DeepFake 코드에서도 VideoFrameDataset에서 이미지 및 라벨을 생성하여야 함.\n",
        "\n",
        "다만, 데이터세트를 분류하는 부분에서 언급한 것 처럼 metadata.json에서 읽어서 라벨을 할당할지, 아니면 별도의 함수로 폴더를 REAL/FAKE로 생성해서 처리할지 결정해서 처리해야함.\n",
        "\n",
        "또한, Cats vs Dogs의 경우 데이터 중 20,000개는 학습에 나머지 4,935개는 검증에 사용했고 학습이나 검증에 대한 라벨을 알 수 있는 상황이기 때문에 검증데이터도 라벨을 할당해서 loss나 accuracy를 확인할 수 있지만 DeepFake의 경우 테스트 데이터에 대해서는 라벨정보가 없기 때문에 모델의 학습이 완료된 후 최종 예측 결과(prediction results)만 얻을 수 있음."
      ],
      "metadata": {
        "id": "l705HacXl_Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CatsVsDogsDataset(Dataset):\n",
        "    def __init__(self, images_filepaths, transform=None):\n",
        "        self.images_filepaths = images_filepaths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_filepaths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.images_filepaths[idx]\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        if os.path.normpath(image_filepath).split(os.sep)[-2] == \"Cat\":\n",
        "            label = 1.0\n",
        "        else:\n",
        "            label = 0.0\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "5LFO7-zKl_i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###학습데이터와 검증데이터의 변환함수를 정의하기 위한 Albumentation 적용\n",
        "\n",
        "아래 예제는 Cats vs Dogs 사례이며 학습하려는 이미지의 특성에 따라서 적정한 augmentation을 해야 함.\n",
        "\n",
        "DeepFake의 경우 1위를 한 사람이 제시한 augmentation을 일단 적용하고 추가로 아이디어가 있다면 추가하여 성능을 비교하여 개선된다면 적용하는 것이 바람직함.\n",
        "동영상에서 추출한 이미지에서 얼굴부분을 잘라내서 분류한다는 점을 감안하여 어떤 증강방식이 좋을지에 대한 아이디어가 필요함."
      ],
      "metadata": {
        "id": "SK3215-Kmajw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.SmallestMaxSize(max_size=160),\n",
        "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
        "        A.RandomCrop(height=128, width=128),\n",
        "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "train_dataset = CatsVsDogsDataset(images_filepaths=train_images_filepaths, transform=train_transform)\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.SmallestMaxSize(max_size=160),\n",
        "        A.CenterCrop(height=128, width=128),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "val_dataset = CatsVsDogsDataset(images_filepaths=val_images_filepaths, transform=val_transform)"
      ],
      "metadata": {
        "id": "IDscGOnumJ6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Augmentation 결과를 표시하는 함수"
      ],
      "metadata": {
        "id": "dej8EK8tmikf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_augmentations(dataset, idx=0, samples=10, cols=5):\n",
        "    dataset = copy.deepcopy(dataset)\n",
        "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
        "    rows = samples // cols\n",
        "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
        "    for i in range(samples):\n",
        "        image, _ = dataset[idx]\n",
        "        ax.ravel()[i].imshow(image)\n",
        "        ax.ravel()[i].set_axis_off()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aUALtcr-mivw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###학습을 위한 Helper function 정의"
      ],
      "metadata": {
        "id": "9aGF2j97mqbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(output, target):\n",
        "    output = torch.sigmoid(output) >= 0.5\n",
        "    target = target == 1.0\n",
        "    return torch.true_divide((target == output).sum(dim=0), output.size(0)).item()"
      ],
      "metadata": {
        "id": "fPWwBvl8mqiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricMonitor:\n",
        "    def __init__(self, float_precision=3):\n",
        "        self.float_precision = float_precision\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
        "\n",
        "    def update(self, metric_name, val):\n",
        "        metric = self.metrics[metric_name]\n",
        "\n",
        "        metric[\"val\"] += val\n",
        "        metric[\"count\"] += 1\n",
        "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \" | \".join(\n",
        "            [\n",
        "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
        "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
        "                )\n",
        "                for (metric_name, metric) in self.metrics.items()\n",
        "            ]\n",
        "        )"
      ],
      "metadata": {
        "id": "3Au6cdjpmzAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###학습 파라메터 설정"
      ],
      "metadata": {
        "id": "AqIFLQHvm1gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"model\": \"resnet50\",\n",
        "    \"device\": \"cuda\",\n",
        "    \"lr\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"num_workers\": 4,\n",
        "    \"epochs\": 10,\n",
        "}"
      ],
      "metadata": {
        "id": "4-7DUvULm1ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###학습과 검증을 위한 모든 함수와 객체 생성  "
      ],
      "metadata": {
        "id": "oQizKwoynByG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "# torchivision v0.13 미만\n",
        "#model = getattr(models, params[\"model\"])(pretrained=False, num_classes=1,)\n",
        "\n",
        "# torchivision v0.13+\n",
        "weights = torchvision.models.ResNet50_Weights.DEFAULT\n",
        "model = torchvision.models.resnet50(weights=weights)\n",
        "model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "\n",
        "model = model.to(params[\"device\"])\n",
        "criterion = nn.BCEWithLogitsLoss().to(params[\"device\"])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])"
      ],
      "metadata": {
        "id": "CXmBknr9nCDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n",
        ")"
      ],
      "metadata": {
        "id": "_AL_8WwKnPIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch, params):\n",
        "    metric_monitor = MetricMonitor()\n",
        "    model.train()\n",
        "    stream = tqdm(train_loader)\n",
        "    for i, (images, target) in enumerate(stream, start=1):\n",
        "        images = images.to(params[\"device\"], non_blocking=True)\n",
        "        print(images.shape) # (64, 3, row, col)\n",
        "        target = target.to(params[\"device\"], non_blocking=True).float().view(-1, 1)\n",
        "        print(target.shape) # (64, 1)\n",
        "        output = model(images)\n",
        "        loss = criterion(output, target)\n",
        "        accuracy = calculate_accuracy(output, target)\n",
        "        metric_monitor.update(\"Loss\", loss.item())\n",
        "        metric_monitor.update(\"Accuracy\", accuracy)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        stream.set_description(\n",
        "            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
        "        )"
      ],
      "metadata": {
        "id": "WHuD3Tq-nVTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_loader, model, criterion, epoch, params):\n",
        "    metric_monitor = MetricMonitor()\n",
        "    model.eval()\n",
        "    stream = tqdm(val_loader)\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(stream, start=1):\n",
        "            images = images.to(params[\"device\"], non_blocking=True)\n",
        "            target = target.to(params[\"device\"], non_blocking=True).float().view(-1, 1)\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "            accuracy = calculate_accuracy(output, target)\n",
        "\n",
        "            metric_monitor.update(\"Loss\", loss.item())\n",
        "            metric_monitor.update(\"Accuracy\", accuracy)\n",
        "            stream.set_description(\n",
        "                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
        "            )"
      ],
      "metadata": {
        "id": "Uj_D56CgnYhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###모델 학습하기  "
      ],
      "metadata": {
        "id": "iSTVakLdnbpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, params[\"epochs\"] + 1):\n",
        "    train(train_loader, model, criterion, optimizer, epoch, params)\n",
        "    validate(val_loader, model, criterion, epoch, params)"
      ],
      "metadata": {
        "id": "9h5DL4qdnbzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###이미지에 대해서 해당 라벨과 그 추정 결과를 표시하기  "
      ],
      "metadata": {
        "id": "pP6LPNh5ng23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CatsVsDogsInferenceDataset(Dataset):\n",
        "    def __init__(self, images_filepaths, transform=None):\n",
        "        self.images_filepaths = images_filepaths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_filepaths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.images_filepaths[idx]\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.SmallestMaxSize(max_size=160),\n",
        "        A.CenterCrop(height=128, width=128),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "test_dataset = CatsVsDogsInferenceDataset(images_filepaths=test_images_filepaths, transform=test_transform)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n",
        ")"
      ],
      "metadata": {
        "id": "33opz5hZnhAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.eval()\n",
        "predicted_labels = []\n",
        "with torch.no_grad():\n",
        "    for images in test_loader:\n",
        "        images = images.to(params[\"device\"], non_blocking=True)\n",
        "        output = model(images)\n",
        "        predictions = (torch.sigmoid(output) >= 0.5)[:, 0].cpu().numpy()\n",
        "        predicted_labels += [\"Cat\" if is_cat else \"Dog\" for is_cat in predictions]"
      ],
      "metadata": {
        "id": "QS8Vy6Tyn1Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image_grid(test_images_filepaths, predicted_labels)"
      ],
      "metadata": {
        "id": "G2Du-rcE5qN7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}